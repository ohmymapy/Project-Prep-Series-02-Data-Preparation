{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzGL_NXtHg-6"
      },
      "source": [
        "# Data preprocessing: more art than science?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMD9pn4nHg-6"
      },
      "source": [
        "## Contents of this notebook:\n",
        "<ol>\n",
        "<li>Load and examine your data</li>\n",
        "<li>Merging two dataframes</li>\n",
        "<li>Removing features that you do not need</li>\n",
        "<li>Making your data machine-readable</li>\n",
        "<li>Handling not available (NA) and inf data</li>\n",
        "<li>Removing columns with a standard deviation of 0</li>\n",
        "<li>Feature scaling</li>\n",
        "<li>Data visualization</li>\n",
        "<li>Loading local files into Google Colab</li>\n",
        "<li>Mini assignment: data visualization</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4-gy6CyHg-7"
      },
      "source": [
        "## 1. Load and examine your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0rrgA3wHg-7"
      },
      "source": [
        "### 1.1 Preliminary examination of your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zoslF_KSHg-7",
        "outputId": "3223af6e-5b8d-425e-ab6f-8d118118bd2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data_csv/unrestricted_HCP_behavioral.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-649a81047e63>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_csv/unrestricted_HCP_behavioral.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_csv/unrestricted_HCP_behavioral.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "df = pd.read_csv(\"data_csv/unrestricted_HCP_behavioral.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr1DKyXvHg-7"
      },
      "source": [
        "Quick quality check: is the dataframe shape what you expect it to be? You should know how many subjects (ie rows) and features (ie columns) you are expecting your dataframe (`df`) to have prior to any downstream analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEt5TwnUHg-7"
      },
      "outputs": [],
      "source": [
        "print(f\"df type: {type(df)}\")\n",
        "print(f\"df shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "FSHLp_iCHg-7"
      },
      "outputs": [],
      "source": [
        "df.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWXYobygHg-7"
      },
      "outputs": [],
      "source": [
        "df.tail(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk4cy6XPHg-8"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrTETeLLHg-8"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSjtp1AWHg-8"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9Bflop5Hg-8"
      },
      "outputs": [],
      "source": [
        "df['Gender'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5EL1XvbHg-8"
      },
      "outputs": [],
      "source": [
        "df['Gender'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3Ot5D0IHg-8"
      },
      "outputs": [],
      "source": [
        "df[df['Gender']==\"F\"] # Display only the rows of female subjects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBF2uJGyHg-8"
      },
      "source": [
        "### 1.2 Documentation\n",
        "Woah! So many columns and abbreviations! What do they all mean? Make sure you know where your [dataset's documentation](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf) is.\n",
        "\n",
        "https://wiki.humanconnectome.org/#resources-for-public-hcp-data-users\n",
        "\n",
        "Unfortunately, thorough documentation is not always available. Some data types are also very field-specific and require the help of experts, which is part of what makes machine learning so wonderfully interdisciplinary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGcj0BmgHg-8"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx1Tmu6RHg-8"
      },
      "source": [
        "Handpicking the columns you want to work with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05ItdALaHg-8"
      },
      "outputs": [],
      "source": [
        "basics = ['Subject','Gender','Age','PSQI_BedTime']\n",
        "df[basics]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXt9VwhIHg-8"
      },
      "source": [
        "Let's also add all of some cognitive variables to the mix! Specifically, we'll select the measures related to fluid intelligence (they start with `PMAT` for Penn Matrix Test) and impulsivity (they start with `DDisc` for Delay Discounting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vPzN98DcHg-8"
      },
      "outputs": [],
      "source": [
        "cognition = ['Subject','Gender','Age','PSQI_BedTime']\n",
        "for col in df.columns:\n",
        "    if (col.find(\"PMAT\")!=-1 or col.find(\"DDisc\")!=-1):\n",
        "        cognition.append(col)\n",
        "print(f\"List of variables we will be looking at: {cognition}\") # PS: f-strings will be very useful for you in your Python journey!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEqYeJ2sHg-8"
      },
      "source": [
        "Okay! So let's actually select this subset of our data, and make it a separate dataframe!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj0jLD_1Hg-9"
      },
      "outputs": [],
      "source": [
        "df_cognition = df[cognition]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "qoHVFasMHg-9"
      },
      "outputs": [],
      "source": [
        "df_cognition.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J6MRU64Hg-9"
      },
      "outputs": [],
      "source": [
        "df_cognition.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxSR034AHg-9"
      },
      "source": [
        "### 2. Merging two dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J4i_eSZHg-9"
      },
      "source": [
        "So for our dataset of 1206 subjects, we have information about their gender, age-range and a variety of cognitive measures. It would be cool to be able to integrate some other data as well. You have been provided with a separate file that contains brain structure volume data obtained from the neuroimaging data of these same subjects (note that not all subjects in the HCP have neuroimaging data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUjID47XHg-9"
      },
      "outputs": [],
      "source": [
        "df_volumes = pd.read_csv(\"data_csv/HCP_volumes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkK1FPLiHg-9"
      },
      "outputs": [],
      "source": [
        "df_volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egewGYe8Hg-9"
      },
      "outputs": [],
      "source": [
        "df_volumes['TBV'].mean() # mean TBV!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in2325PDHg-9"
      },
      "outputs": [],
      "source": [
        "df_volumes['TBV'].std() # standard deviation of TBV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Cu38eAHg-9"
      },
      "source": [
        "List all our subjects whose TBV is above average! (PS: another way to write this: `df_volumes[df_volumes['TBV']>1406181]`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24zcAcroHg-9"
      },
      "outputs": [],
      "source": [
        "df_volumes[df_volumes['TBV']>df_volumes['TBV'].mean()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR7XmeScHg-9"
      },
      "source": [
        "Ok! Let's merge our dataframes. Do they have the same number of subjects?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "C-dApf8JHg-9"
      },
      "outputs": [],
      "source": [
        "print(df_volumes.shape)\n",
        "print(df_cognition.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fONH2eoBHg-9"
      },
      "source": [
        "No! We only have volume data for 1086 subjects... So how do we create a new dataframe, where we only keep the subjects for which we have all of our features?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnmxYso7Hg-9"
      },
      "outputs": [],
      "source": [
        "df_final=pd.merge(df_cognition,df_volumes, on='Subject')\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XVE-YvuHg-9"
      },
      "outputs": [],
      "source": [
        "df_final.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAjb-DcLHg-9"
      },
      "source": [
        "Mission accomplished! We have 1086 and 14 + 22 features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aSnt-twHg--"
      },
      "source": [
        "## 3. Removing features that you do not need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jqypQmZzHg--"
      },
      "outputs": [],
      "source": [
        "df_final.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz_yA9a2Hg--"
      },
      "source": [
        "Do we have any columns that repeat themselves? Yes we do! `DDisc_AUC_200` and `DDisc_AUC_200.1` might be the same. Let's check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLxOdJcnHg--"
      },
      "outputs": [],
      "source": [
        "df_final[\"DDisc_AUC_200\"].equals(df_final[\"DDisc_AUC_200.1\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_final.columns:\n",
        "  for col2 in df_final.columns:\n",
        "    if(col != col2):\n",
        "      if df_final[col].equals(df_final[col2])\n",
        "      # now compare contents of 2 col\n",
        "      print(col,col2)"
      ],
      "metadata": {
        "id": "wdC2J1BqKjpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAXlTN-EHg--"
      },
      "source": [
        "Nice! Let's remove column `DDisc_AUC_200.1` then."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "l9VogYtVHg--"
      },
      "outputs": [],
      "source": [
        "df_final.drop('DDisc_AUC_200.1', inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFELC-p5Hg--"
      },
      "outputs": [],
      "source": [
        "df_final.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scEtdk-MHg--"
      },
      "source": [
        "## 4. Making your data machine-readable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_NJx7WoHg--"
      },
      "source": [
        "To be machine-readable, your variables need to be numerical. Want to check?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtWX_oEEHg--"
      },
      "outputs": [],
      "source": [
        "df_final.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0T9v7inHg--"
      },
      "source": [
        "We have 3 columns that are non-numerical: `Gender`,`Age`,`PSQI_BedTime`. Let's figure out how to handle them, one at a time. First of all, we know that the `Gender` column is categorical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t__nhhw2Hg--"
      },
      "outputs": [],
      "source": [
        "df_final['Gender'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oqPQIEvHg--"
      },
      "source": [
        "### 4.1 One-hot encoding or binarizing your data\n",
        "In this specific dataset, the `Gender` column has one of two values: `M` or `F`. Given that we only have two categories here, you can just replace all of your `M` values with 1 and `F` values with 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH0tiGftHg--"
      },
      "source": [
        "Indeed, it has one of two values: `M` or `F`. Given that we only have two categories here, you can easily just set all of the `M`s to 1 and the `F` to 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJrfjiabHg--"
      },
      "outputs": [],
      "source": [
        "df_final['Gender'] = df_final['Gender'].replace('M',1)\n",
        "df_final['Gender'] = df_final['Gender'].replace('F',2)\n",
        "df_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbgmV-R1Hg--"
      },
      "source": [
        "However, suppose that you actually had more than 2 numerical values for this feature (e.g. `M`,`F`,`other`). If you just convert categorical variables to numerical values (ex: `M`=1,`F`=2,`other`=3), you give a \"distance\" to the relationship between variables. For instance, since 1 is closer to 2 than to 3, you are telling your machine that `M` is \"closer\" to `F` (`distance = 2 - 1 = 1`) than to `other` (`distance = 3 - 1 = 2`). We want our categories to be independent. That's where one hot encoding comes into play: \"[A representation of categorical variables as binary vectors](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)\"\\\n",
        "\n",
        "Let's revert our `Gender` feature back to its original form, and see how to one-hot encode it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lp82SPtHg--"
      },
      "outputs": [],
      "source": [
        "df_final['Gender'] = df_final['Gender'].replace(1,'M')\n",
        "df_final['Gender'] = df_final['Gender'].replace(2,'F')\n",
        "\n",
        "one_hot = pd.get_dummies(df_final['Gender'])\n",
        "one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQQxFYtSHg-_"
      },
      "source": [
        "As you can see, we convert 1 categorical feature into N binary features, where N = number of values that your feature can take on (2 in this example). Let's add it to our dataframe!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlPUeCAUHg-_"
      },
      "outputs": [],
      "source": [
        "df_final.join(one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPYeJ7obHg-_"
      },
      "outputs": [],
      "source": [
        "df_final.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdsAjq62Hg-_"
      },
      "source": [
        "Make sure you reassign the modification you made to `df_final`, or it will not save!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMk1sq_WHg-_"
      },
      "outputs": [],
      "source": [
        "df_final = df_final.join(one_hot)\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poWzpr1IHg-_"
      },
      "outputs": [],
      "source": [
        "df_final = df_final.drop('Gender',axis = 1)  # Drop column Gender as it is now one-hot encoded\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx8AR2ZUHg-_"
      },
      "source": [
        "Now, several people have been wondering... What happens if you have a categorical variables with a LOT of different possible values (e.g. country data). Wouldn't one-hot encoding induce the problem of sparsity (i.e. a matrix with too many 0s and 1s)? Indeed, this could be an issue. Kate Nimegeers however had a really creative idea: one could theoretically replace the \"Country\" variables with 2 variables for the latitude and longitude of the centroid of each country in the dataset (or one could find a way to create some \"aggregate measure\" of latitude and longitude). Thus, there is lots of room for creativity :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG8okQ4LHg-_"
      },
      "source": [
        "### 4.2 Parsing strings in your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDFWyRchHg-_"
      },
      "source": [
        "#### 4.2.1 Handling the `Age` feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDFCTVjVHg-_"
      },
      "source": [
        "Recall: our `Age` feature is also non-numerical! Luckily, we have an age range, so the string can be split up into two new columns: min age and max age. Let's replace our age range in `df_final` with an average age estimate for each subject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIjn_0NgHg-_"
      },
      "outputs": [],
      "source": [
        "df_final['Age'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiIuUBEAHg-_"
      },
      "source": [
        "For the sake of this exercise, let's replace 36+ with a range of 36-40."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPq8WiegHg-_"
      },
      "outputs": [],
      "source": [
        "df_final['Age'] = df_final['Age'].replace(\"36+\",'36-40')\n",
        "df_final['Age'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YLKoieHHg-_"
      },
      "outputs": [],
      "source": [
        "fix_age = df_final['Age'].str.split('-', 1, expand=True)\n",
        "fix_age.columns = ['min','max']\n",
        "fix_age\n",
        "# fix_age['mean'] = (fix_age['max']+fix_age['min'])/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW-mtrt2Hg-_"
      },
      "outputs": [],
      "source": [
        "fix_age[\"min\"] = fix_age['min'].astype(float)\n",
        "fix_age[\"max\"] = fix_age['max'].astype(float)\n",
        "fix_age['mean'] = (fix_age['max']+fix_age['min'])/2\n",
        "fix_age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8-bsDppHg-_"
      },
      "outputs": [],
      "source": [
        "df_final['Age'] = fix_age['mean']\n",
        "df_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lLnCguPHg-_"
      },
      "source": [
        "#### 4.2.2 Handling the `PSQI_BedTime` feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8yuNrqrHg-_"
      },
      "source": [
        "Convert your bed time variable from HH:MM:SS to seconds!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl2BgpdXHg-_"
      },
      "outputs": [],
      "source": [
        "df_final['PSQI_BedTime']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JYbgKlAeHg-_"
      },
      "outputs": [],
      "source": [
        "ftr = [3600,60,1]\n",
        "for i in range(len(df_final['PSQI_BedTime'])):\n",
        "    x = sum([a*b for a,b in zip(ftr, map(int,df_final['PSQI_BedTime'][i].split(':')))])\n",
        "    df_final['PSQI_BedTime'][i] = x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VTYgfBNZHg_A"
      },
      "outputs": [],
      "source": [
        "df_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JAp2kQrHg_A"
      },
      "source": [
        "#### 4.2.3 Other strings that often crop up in dataframes and need to be replaced with numbers!\n",
        "`\n",
        "df_final = df_final.replace('FALSE',0)\n",
        "df_final = df_final.replace('TRUE',1)\n",
        "df_final = df_final.replace(False,0)\n",
        "df_final = df_final.replace(True,1)\n",
        "df_final = df_final.replace('0',0) # example of random spaces\n",
        "df_final = df_final.replace(' ',np.NaN) # example of random spaces\n",
        "`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZCz0UnVHg_A"
      },
      "source": [
        "#### 4.2.4 Making sure every column is of type float\n",
        "The next line of code will throw an error if you forgot to replace any strings, and it will tell you what those strings are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KArHbYOsHg_A"
      },
      "outputs": [],
      "source": [
        "for col in df_final.columns:\n",
        "    df_final[col] = df_final[col].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3e3i4T2Hg_A"
      },
      "outputs": [],
      "source": [
        "df_final.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbTrA7vmHg_A"
      },
      "source": [
        "## 5. Handling not available (NA) and inf data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAtLU682Hg_A"
      },
      "source": [
        "Sometimes, Python will convert some of your values to + or - infinity, which will result in downstream errors. Convert them to NA, and then handle them as NA values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD7hCckvHg_A"
      },
      "outputs": [],
      "source": [
        "df_final = df_final.replace([np.inf, -np.inf], np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdUCpqUaHg_A"
      },
      "source": [
        "Next, you need to deal with your NA values. How many nas do you have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T-d19yLHg_A"
      },
      "outputs": [],
      "source": [
        "df_final.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eMzdNvJHg_A"
      },
      "source": [
        "There is a variety of ways to handle `na` data. The most simple approach is to replace `na` data with the median (or mean) value of the feature of interest. There are [other](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779) [more](https://arxiv.org/abs/1804.11087) sophisticated data imputation techniques out there, many of which actually leverage machine learning tools (so meta)!\n",
        "\n",
        "However, if a feature has too many NAs, you may want to remove it completely. Define a threshold for the minimal number of missing values that qualifies a feature for removal from your dataset. Here, 12 is quite stringent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsseC1L3Hg_A"
      },
      "outputs": [],
      "source": [
        "threshold=12\n",
        "\n",
        "remove_cols = []\n",
        "for i in range(len(df_final.columns)):\n",
        "    if (df_final.iloc[:,i].isnull().sum() >= threshold):\n",
        "        remove_cols.append(df_final.columns[i])\n",
        "df_final = df_final.drop(columns=remove_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guMSskCgHg_A"
      },
      "source": [
        "Next, let's replace the NA values we have left with the feature-specific median (the median is more robust against outliers than the mean is)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSWpUrDWHg_A"
      },
      "outputs": [],
      "source": [
        "for col in df_final.columns:\n",
        "    df_final[col].fillna(df_final[col].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVf_JQ5nHg_A"
      },
      "outputs": [],
      "source": [
        "df_final.isna().sum().sum()\n",
        "# note the difference between df_final.isna().sum() and df_final.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3mRb1GcHg_A"
      },
      "source": [
        "## 6. Removing columns with a standard deviation of 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15xh71HhHg_A"
      },
      "outputs": [],
      "source": [
        "df_final.std()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_final.columns:\n",
        "  df_final[col].fillna(df_final[col].meadian(), inplace=True)"
      ],
      "metadata": {
        "id": "ulHwaBmOTUhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEB_MhxYHg_A"
      },
      "outputs": [],
      "source": [
        "df_final = df_final.loc[:, df_final.std() > 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZf0nEVSHg_A"
      },
      "source": [
        "## 7. Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMUc32ekHg_A"
      },
      "source": [
        "You usually need to perform some sort of feature scaling to make sure that all of your variables are in the same range (this affects gradient-descent-based algorithms and distance-based algorithms==).\n",
        "\n",
        "**Min-Max Scaling / Normalization:** X' = (X-Xmin) / (Xmax-Xmin), X' always ends up with a range of \\[0,1\\] \\\n",
        "**Standardization / Standard Scaler / Z-score):** X' = (X-mu)/sigma\n",
        "\n",
        "Which to use? Depends on your data! [\"Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution.\"](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/). Other [popular scaling techniques](https://www.analyticsvidhya.com/blog/2020/07/types-of-feature-transformation-and-scaling/) include the log transform (you often see this with GWAS, ie genome-wide association studies) and dividing your column-wise values by the absolute value of the maximal value of each column (max abs scaler)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScUucBm_Hg_B"
      },
      "source": [
        "Example 1: Min-Max Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0yWd_2fHg_B"
      },
      "outputs": [],
      "source": [
        "minMaxScaled_df_final=(df_final-df_final.min())/(df_final.max()-df_final.min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UtkuOGHHg_B"
      },
      "source": [
        "Example 2: Standard Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQEAwgsGHg_B"
      },
      "outputs": [],
      "source": [
        "standardized_df_final=(df_final-df_final.mean())/df_final.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0NEC0iHHg_B"
      },
      "source": [
        "Example 3: Sklearn Min-Max Scaler\\\n",
        "Slightly different from the Min-Max Scaling defined above:\\\n",
        "`\n",
        "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
        "X_scaled = X_std * (max - min) + min\n",
        "`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMu3DPPwHg_B"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mms=MinMaxScaler()\n",
        "mms.fit(df_final)\n",
        "df_final_mms=mms.transform(df_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPVAUkDaHg_B"
      },
      "source": [
        "# 8. Data visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmaDuKFuHg_B"
      },
      "source": [
        "Data visualization is a wonderful way to get to know your data in order to plan a relevant analysis or find an appropriate machine learning application. [Matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/) are two canonical data visualizations tools that you can use in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMFjpflRHg_B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds90ViFxHg_B"
      },
      "source": [
        "#### 8.1 Scatter plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppWii9BXHg_B"
      },
      "outputs": [],
      "source": [
        "plt.scatter(df_final[\"PSQI_BedTime\"],df_final[\"TBV\"])\n",
        "plt.ylabel(\"Total brain volume (mm3)\")\n",
        "plt.xlabel(\"Bedtime (seconds)\")\n",
        "plt.title(\"Total brain volume as a function of bed time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS8QjbWfHg_B"
      },
      "source": [
        "#### 8.2 [Violin plot](https://chartio.com/learn/charts/violin-plot-complete-guide/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLdCN-XCHg_B"
      },
      "outputs": [],
      "source": [
        "plt.violinplot(df_final['PSQI_BedTime'])\n",
        "plt.ylabel(\"Bed time\")\n",
        "plt.title(\"Violin plot of bed times in our sample\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9wqJf3fHg_B"
      },
      "source": [
        "#### 8.3 Histogram plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgIgeCB8Hg_B"
      },
      "outputs": [],
      "source": [
        "sns.displot(df_final,x='TBV',kind='kde',fill=True) # smoothed histogram\n",
        "plt.ylabel(\"Density\")\n",
        "plt.xlabel(\"Total brain volume (mm3)\")\n",
        "plt.title(\"Distribution of total brain volume in our sample\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tycZwrbHg_B"
      },
      "outputs": [],
      "source": [
        "sns.histplot(df_final,x='Str_Left',fill=True,color='orange')\n",
        "sns.histplot(df_final,x='Thal_Left',fill=True,color='turquoise')\n",
        "plt.legend(['Left striatum','Left thalamus'])\n",
        "plt.ylabel(\"Density\")\n",
        "plt.xlabel(\"Structure-specific volume (mm3)\")\n",
        "plt.title(\"Distribution of different structure volumes in our sample\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifVBMefAHg_B"
      },
      "source": [
        "# 9. Loading local files into Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0hsFE25Hg_B"
      },
      "source": [
        "Nice tutorial [here](https://neptune.ai/blog/google-colab-dealing-with-files)\n",
        "\n",
        "**TLDR:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3eHGSy7Hg_B"
      },
      "outputs": [],
      "source": [
        "from google.colab import files # files module from google.colab library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJVm2eOGHg_B"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload # you can then select files and upload them to your colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKvnsqlxHg_B"
      },
      "source": [
        "# 10. Mini data visualization assignment\n",
        "PS: this [list of named colours](https://matplotlib.org/3.5.0/gallery/color/named_colors.html) can help make your graphs nicer :)\n",
        "<ol>\n",
        "<li>Generate a scatter plot of TBV as a function of bed time, by gender.</li>\n",
        "<li>Generate a violin plot of bed times in our sample for subjects over 30 years.</li>\n",
        "<li>Generate two smoothed and superimposed histograms of bed times in subjects that are above and below 30 years.</li>\n",
        "<li>Generate two superimposed histograms of the volume distributions in the left hemisphere (left striatum, thalamus and globus pallidus) and right hemisphere (right striatum, thalamus and globus pallidus)</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQHvTAPBHg_C"
      },
      "source": [
        "# Supplementary\n",
        "\n",
        "### Interactive plots with Plotly Python\n",
        "\n",
        "* FYI: Plotly is based in Montreal!\n",
        "* [Superb documentation](https://plotly.com/python/)\n",
        "* Plotly express is more \"higher-level\" (tons of parameters are already set for you); can save you time but is less flexible\n",
        "\n",
        "### Bringing it to the next level: making an App with Dash\n",
        "* Documentation [here](https://dash.plotly.com/)\n",
        "* html + Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P4p4iPlHg_C"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}